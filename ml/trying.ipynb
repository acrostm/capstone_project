{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f994fe58-ae8b-4fed-ab99-5824686df256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bedb1a-01bb-4dc5-a399-2b8adfa76aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Mediapipe Pose 模型\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fba0964-4bb3-488c-955a-1d684ce8af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function detects human pose estimation keypoints from webcam footage\n",
    "    \n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f873fbff-0def-41df-9756-84c193be3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "    \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90287765-140a-478d-ac4c-a1e67e263573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5f92f2-d2fd-4bb2-ac65-f9a402c64371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径和参数设置（保持不变）\n",
    "DATA_PATH = os.path.join(os.getcwd(),'data')\n",
    "actions = np.array(['curl', 'squats', 'bridges'])\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "no_sequences = 15\n",
    "start_folder = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c4be2bb-f7cc-4c9e-b1b7-4e9e62bc46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置每个视频序列的帧数\n",
    "sequence_length =30  # 假设每个视频序列包含 30 帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd7d127-1094-476f-83eb-1982d107eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据文件夹\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "for action in actions:\n",
    "    dir_path = os.path.join(DATA_PATH, action)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    for sequence in range(no_sequences):\n",
    "        sequence_path = os.path.join(dir_path, str(sequence))\n",
    "        if not os.path.exists(sequence_path):\n",
    "            os.makedirs(sequence_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beafcf65-2b9a-4296-b2a2-87c8139ee1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files = {'curl': 'videos/curls.mp4', 'squats': 'videos/squats.mp4', 'bridges': 'videos/bridges.mp4'}  # 替换为实际视频路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e830c0-0ef0-43f0-b76c-af8cd88d5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710786634.719171       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    for action, video_file in video_files.items():\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frame_count = 0\n",
    "        sequence = 0\n",
    "\n",
    "        while cap.isOpened() and sequence < no_sequences:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            image, results = mediapipe_detection(frame, pose)\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "            frame_count += 1\n",
    "            if frame_count == sequence_length:\n",
    "                sequence += 1\n",
    "                frame_count = 0\n",
    "\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5f47c-3adf-4248-ad0f-91e98ec6b291",
   "metadata": {},
   "source": [
    "processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bd51c57-62b0-414b-bb30-6a8138a8f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7f10f26-fd6b-4732-9f89-1b85ca0e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    # 获取动作文件夹的路径\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    \n",
    "    # 过滤掉非数字的文件夹名（排除如 .DS_Store 的文件）\n",
    "    sequence_dirs = [dir_name for dir_name in os.listdir(action_path) if dir_name.isdigit()]\n",
    "\n",
    "    # 现在可以安全地将目录名转换为整数\n",
    "    for sequence in np.array(sequence_dirs).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):         \n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)  \n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a19e70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Map: {'curl': 0, 'squats': 1, 'bridges': 2}\n",
      "Labels Sample: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total number of labels: 45\n"
     ]
    }
   ],
   "source": [
    "# 检查 label_map 是否包含所有动作\n",
    "print(\"Label Map:\", label_map)\n",
    "\n",
    "# 在加载数据后，检查 labels 列表\n",
    "print(\"Labels Sample:\", labels[:15])  # 打印前10个标签作为样本\n",
    "\n",
    "# 检查 labels 列表的长度是否与数据点总数相匹配\n",
    "print(\"Total number of labels:\", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3877eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: {0: 15, 1: 15, 2: 15}\n"
     ]
    }
   ],
   "source": [
    "# 计算每个类别的样本数量\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Class counts:\", class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7d8d5ac-f56b-45ee-b30f-e4fd571ec603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f051ef1-9c5a-4bb9-ad0a-9c87d4f58f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded labels: (45, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels_array = np.array(labels).reshape(-1, 1)\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(labels_array).toarray()  # 使用 toarray() 转换为密集矩阵\n",
    "X = np.array(sequences)\n",
    "print(\"Shape of encoded labels:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c2d333e-d286-47c2-ae8d-dee058e553e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (31, 30, 132) (31, 3)\n",
      "Testing set: (14, 30, 132) (14, 3)\n"
     ]
    }
   ],
   "source": [
    "# 分割数据集为训练集、验证集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)\n",
    "\n",
    "# 输出数据集的形状以确认\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "#print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training data: [0. 1.]\n",
      "Unique labels in test data: [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels_train = np.unique(y_train)\n",
    "unique_labels_test = np.unique(y_test)\n",
    "print(\"Unique labels in training data:\", unique_labels_train)\n",
    "print(\"Unique labels in test data:\", unique_labels_test)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "995673a6-6119-4983-bc40-e01c035a0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3337943-9754-464f-bb4f-14f272cac662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_size: torch.Size([14, 30, 132])\n",
      "test_data_size: torch.Size([14, 30, 132])\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "test_label_size: torch.Size([14, 3])\n",
      "n_data_size_test: 14\n",
      "train_data_size: torch.Size([31, 30, 132])\n",
      "train_label_size: torch.Size([31, 3])\n",
      "n_data_size_train: 31\n"
     ]
    }
   ],
   "source": [
    "tensor_X_test = torch.from_numpy(X_test)\n",
    "print('test_data_size:', tensor_X_test.size())\n",
    "tensor_X_test = torch.from_numpy(X_test)\n",
    "print('test_data_size:',tensor_X_test.size())\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "tensor_y_test = torch.from_numpy(y_test)\n",
    "print('test_label_size:',tensor_y_test.size())\n",
    "n_data_size_test = tensor_X_test.size()[0]\n",
    "\n",
    "\n",
    "\n",
    "print('n_data_size_test:',n_data_size_test)\n",
    "\n",
    "tensor_X_train = torch.from_numpy(X_train)\n",
    "print('train_data_size:',tensor_X_train.size())\n",
    "tensor_y_train = torch.from_numpy(y_train)\n",
    "print('train_label_size:',tensor_y_train.size())\n",
    "n_data_size_train = tensor_X_train.size()[0]\n",
    "print('n_data_size_train:',n_data_size_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98194093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e0a6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设您的标签数组是 tensor_y_train\n",
    "unique_labels = np.unique(tensor_y_train.numpy())\n",
    "print(\"Unique labels in the dataset:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "413c330e-d6d6-4390-9f4c-348cd357ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim,hidden_dim,layer_num,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        self.bn = nn.BatchNorm1d(30)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.bn(inputs)\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c86b4c6e-147c-4351-aa61-b23fe73321c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(132, 128, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (bn): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128  # 隐藏层维度\n",
    "n_joints = 132  # 输入维度，每帧的特征数量\n",
    "n_categories = 3  # 输出维度，类别数量\n",
    "n_layer = 3  # LSTM 层数\n",
    "rnn = LSTM(n_joints,n_hidden,n_categories,n_layer)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0e0507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['curls','squats','bridges']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad95cfd6-ff38-472b-ac7f-083a89016107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return LABELS[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cce670-647b-45b6-bfe6-30c7485220f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec88ddac-5ae3-43ae-ac48-8a7d327a3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def randomTrainingExampleBatch(batch_size, flag, num=-1):\n",
    "    if flag == 'train':\n",
    "        X = tensor_X_train\n",
    "        y = tensor_y_train\n",
    "        data_size = n_data_size_train\n",
    "    elif flag == 'test':\n",
    "        X = tensor_X_test\n",
    "        y = tensor_y_test\n",
    "        data_size = n_data_size_test\n",
    "    \n",
    "    # 确保不会尝试从小于批量大小的数据集中提取数据\n",
    "    if data_size < batch_size:\n",
    "        raise ValueError(\"Data size is smaller than batch size\")\n",
    "    \n",
    "    if num == -1:\n",
    "        ran_num = random.randint(0, data_size - batch_size)\n",
    "    else:\n",
    "        ran_num = num\n",
    "    \n",
    "    # print(\"ran_num_2: %d, %d, %d\" % (ran_num, data_size - batch_size, data_size))\n",
    "    pose_sequence_tensor = X[ran_num:(ran_num + batch_size)]\n",
    "    category_tensor = y[ran_num:ran_num + batch_size, :]\n",
    "    return category_tensor.long(), pose_sequence_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7df1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eb093ea-6974-4f7c-a554-5b77cbf35f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "learning_rate = 0.0005\n",
    "optimizer = optim.SGD(rnn.parameters(),lr=learning_rate,momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f351bad1-9629-4ef3-bbf2-7b80debdb6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1% (30m 33s) 0.0009  / bridges ✓\n",
      "2000 2% (30m 50s) 0.0007  / curls ✓\n",
      "3000 3% (31m 6s) 0.0006  / squats ✓\n",
      "4000 4% (31m 24s) 0.0006  / squats ✓\n",
      "5000 5% (31m 41s) 0.0005  / squats ✓\n",
      "6000 6% (31m 58s) 0.0005  / squats ✓\n",
      "7000 7% (32m 16s) 0.0005  / squats ✓\n",
      "8000 8% (32m 32s) 0.0004  / squats ✓\n",
      "9000 9% (32m 49s) 0.0005  / curls ✓\n",
      "10000 10% (33m 7s) 0.0005  / curls ✓\n",
      "11000 11% (33m 24s) 0.0004  / bridges ✓\n",
      "12000 12% (33m 42s) 0.0005  / squats ✓\n",
      "13000 13% (33m 58s) 0.0004  / curls ✓\n",
      "14000 14% (34m 16s) 0.0005  / curls ✓\n",
      "15000 15% (34m 33s) 0.0004  / curls ✓\n",
      "16000 16% (34m 49s) 0.0004  / bridges ✓\n",
      "17000 17% (35m 5s) 0.0004  / curls ✓\n",
      "18000 18% (35m 22s) 0.0004  / squats ✓\n",
      "19000 19% (35m 37s) 0.0004  / curls ✓\n",
      "20000 20% (35m 53s) 0.0003  / squats ✓\n",
      "21000 21% (36m 10s) 0.0003  / curls ✓\n",
      "22000 22% (36m 26s) 0.0003  / curls ✓\n",
      "23000 23% (36m 44s) 0.0003  / curls ✓\n",
      "24000 24% (37m 0s) 0.0004  / bridges ✓\n",
      "25000 25% (37m 17s) 0.0003  / squats ✓\n",
      "26000 26% (37m 35s) 0.0004  / bridges ✓\n",
      "27000 27% (37m 52s) 0.0002  / squats ✓\n",
      "28000 28% (38m 9s) 0.0003  / squats ✓\n",
      "29000 28% (38m 26s) 0.0002  / curls ✓\n",
      "30000 30% (38m 43s) 0.0003  / bridges ✓\n",
      "31000 31% (38m 59s) 0.0003  / curls ✓\n",
      "32000 32% (39m 17s) 0.0002  / squats ✓\n",
      "33000 33% (39m 35s) 0.0002  / squats ✓\n",
      "34000 34% (39m 51s) 0.0002  / bridges ✓\n",
      "35000 35% (40m 8s) 0.0002  / squats ✓\n",
      "36000 36% (40m 25s) 0.0002  / curls ✓\n",
      "37000 37% (40m 41s) 0.0002  / curls ✓\n",
      "38000 38% (40m 58s) 0.0002  / squats ✓\n",
      "39000 39% (41m 15s) 0.0002  / squats ✓\n",
      "40000 40% (41m 31s) 0.0002  / squats ✓\n",
      "41000 41% (41m 48s) 0.0002  / bridges ✓\n",
      "42000 42% (42m 4s) 0.0002  / bridges ✓\n",
      "43000 43% (42m 21s) 0.0002  / curls ✓\n",
      "44000 44% (42m 38s) 0.0002  / bridges ✓\n",
      "45000 45% (42m 55s) 0.0002  / squats ✓\n",
      "46000 46% (43m 12s) 0.0003  / bridges ✓\n",
      "47000 47% (43m 28s) 0.0002  / squats ✓\n",
      "48000 48% (43m 45s) 0.0002  / curls ✓\n",
      "49000 49% (44m 2s) 0.0002  / squats ✓\n",
      "50000 50% (44m 18s) 0.0002  / bridges ✓\n",
      "51000 51% (44m 35s) 0.0001  / curls ✓\n",
      "52000 52% (44m 52s) 0.0002  / bridges ✓\n",
      "53000 53% (45m 8s) 0.0002  / curls ✓\n",
      "54000 54% (45m 25s) 0.0002  / bridges ✓\n",
      "55000 55% (45m 42s) 0.0002  / squats ✓\n",
      "56000 56% (45m 58s) 0.0002  / bridges ✓\n",
      "57000 56% (46m 16s) 0.0002  / curls ✓\n",
      "58000 57% (46m 33s) 0.0002  / curls ✓\n",
      "59000 59% (46m 51s) 0.0002  / bridges ✓\n",
      "60000 60% (47m 9s) 0.0001  / curls ✓\n",
      "61000 61% (47m 25s) 0.0002  / bridges ✓\n",
      "62000 62% (47m 43s) 0.0001  / squats ✓\n",
      "63000 63% (48m 0s) 0.0001  / squats ✓\n",
      "64000 64% (48m 18s) 0.0001  / squats ✓\n",
      "65000 65% (48m 36s) 0.0001  / bridges ✓\n",
      "66000 66% (48m 55s) 0.0002  / bridges ✓\n",
      "67000 67% (49m 14s) 0.0001  / bridges ✓\n",
      "68000 68% (49m 32s) 0.0001  / curls ✓\n",
      "69000 69% (49m 48s) 0.0001  / curls ✓\n",
      "70000 70% (50m 13s) 0.0001  / squats ✓\n",
      "71000 71% (50m 35s) 0.0001  / bridges ✓\n",
      "72000 72% (51m 0s) 0.0001  / curls ✓\n",
      "73000 73% (51m 26s) 0.0002  / squats ✓\n",
      "74000 74% (51m 46s) 0.0001  / curls ✓\n",
      "75000 75% (52m 6s) 0.0001  / bridges ✓\n",
      "76000 76% (52m 37s) 0.0001  / curls ✓\n",
      "77000 77% (53m 6s) 0.0001  / curls ✓\n",
      "78000 78% (53m 37s) 0.0001  / curls ✓\n",
      "79000 79% (53m 57s) 0.0001  / bridges ✓\n",
      "80000 80% (54m 16s) 0.0001  / squats ✓\n",
      "81000 81% (54m 34s) 0.0001  / curls ✓\n",
      "82000 82% (54m 54s) 0.0001  / curls ✓\n",
      "83000 83% (55m 17s) 0.0001  / squats ✓\n",
      "84000 84% (55m 39s) 0.0001  / curls ✓\n",
      "85000 85% (56m 0s) 0.0001  / curls ✓\n",
      "86000 86% (56m 19s) 0.0001  / curls ✓\n",
      "87000 87% (56m 38s) 0.0001  / bridges ✓\n",
      "88000 88% (56m 57s) 0.0001  / bridges ✓\n",
      "89000 89% (57m 17s) 0.0001  / bridges ✓\n",
      "90000 90% (57m 36s) 0.0001  / squats ✓\n",
      "91000 91% (57m 55s) 0.0001  / curls ✓\n",
      "92000 92% (58m 16s) 0.0001  / squats ✓\n",
      "93000 93% (58m 37s) 0.0001  / squats ✓\n",
      "94000 94% (58m 56s) 0.0001  / squats ✓\n",
      "95000 95% (59m 15s) 0.0001  / squats ✓\n",
      "96000 96% (59m 34s) 0.0001  / squats ✓\n",
      "97000 97% (59m 54s) 0.0001  / bridges ✓\n",
      "98000 98% (60m 16s) 0.0001  / squats ✓\n",
      "99000 99% (60m 40s) 0.0001  / curls ✓\n",
      "100000 100% (61m 6s) 0.0001  / bridges ✓\n"
     ]
    }
   ],
   "source": [
    "n_iters = 100000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "batch_size = 5\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category_tensor, input_sequence = randomTrainingExampleBatch(batch_size, 'train')\n",
    "    input_sequence = input_sequence.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "  \n",
    "\n",
    "\n",
    "    # 获取类别索引\n",
    "    _, category_tensor = category_tensor.max(dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = rnn(input_sequence.float())\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    current_loss += loss.item()\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == LABELS[category_tensor[0]] else '✗ (%s)' % LABELS[category_tensor[0]]\n",
    "        print('%d %d%% (%s) %.4f  / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75131e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10d76bd4-afb9-4fd8-9f7a-6308676a27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(),'lstm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87398f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(flag):\n",
    "    if flag == 'train':\n",
    "        n = n_data_size_train\n",
    "    elif flag == 'test':\n",
    "        n = n_data_size_test\n",
    "\n",
    "    with torch.no_grad():\n",
    "        right = 0\n",
    "        start = time.time()\n",
    "        for i in range(n):\n",
    "            category_tensor, inputs = randomTrainingExampleBatch(1, flag, i)\n",
    "            category_index = category_tensor[0].nonzero(as_tuple=True)[0].item()\n",
    "            category = LABELS[category_index]\n",
    "\n",
    "            inputs = inputs.to(device).float()\n",
    "            output = rnn(inputs)\n",
    "            \n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess_i == category_index else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f / %s %s' % (i + 1, (i + 1) / n * 100, timeSince(start), output[0][guess_i], guess, correct))\n",
    "            \n",
    "            if category_index == guess_i:\n",
    "                right += 1\n",
    "\n",
    "    print(flag, 'accuracy', right / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e34bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(flag):\n",
    "#     if flag == 'train':\n",
    "#         n = n_data_size_train\n",
    "#     elif flag == 'test':\n",
    "#         n = n_data_size_test\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         right = 0\n",
    "#         for i in range(n):\n",
    "#             category_tensor, inputs = randomTrainingExampleBatch(1, flag, i)\n",
    "#             category_index = category_tensor[0].nonzero(as_tuple=True)[0].item()\n",
    "#             category = LABELS[category_index]\n",
    "\n",
    "#             # 确保 inputs 在正确的设备上，并且是正确的数据类型\n",
    "#             inputs = inputs.to(device).float()\n",
    "#             output = rnn(inputs)\n",
    "            \n",
    "#             guess, guess_i = categoryFromOutput(output)\n",
    "#             if category_index == guess_i:\n",
    "#                 right += 1\n",
    "\n",
    "#     print(flag, 'accuracy', right / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f075e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7% (0m 0s) 6.8801 / curls ✓\n",
      "2 14% (0m 0s) 6.8849 / curls ✓\n",
      "3 21% (0m 0s) 6.2443 / bridges ✓\n",
      "4 28% (0m 0s) 6.4526 / squats ✓\n",
      "5 35% (0m 0s) 6.4690 / squats ✓\n",
      "6 42% (0m 0s) 6.2389 / bridges ✓\n",
      "7 50% (0m 0s) 6.4604 / squats ✓\n",
      "8 57% (0m 0s) 6.2370 / bridges ✓\n",
      "9 64% (0m 0s) 6.2499 / bridges ✓\n",
      "10 71% (0m 0s) 6.2500 / bridges ✓\n",
      "11 78% (0m 0s) 6.2516 / bridges ✓\n",
      "12 85% (0m 0s) 6.2468 / bridges ✓\n",
      "13 92% (0m 0s) 6.4589 / squats ✓\n",
      "14 100% (0m 0s) 6.4662 / squats ✓\n",
      "test accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "test(flag='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dcfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(flag='train')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
