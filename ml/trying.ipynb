{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f994fe58-ae8b-4fed-ab99-5824686df256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38bedb1a-01bb-4dc5-a399-2b8adfa76aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Mediapipe Pose 模型\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fba0964-4bb3-488c-955a-1d684ce8af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function detects human pose estimation keypoints from webcam footage\n",
    "    \n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f873fbff-0def-41df-9756-84c193be3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "    \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90287765-140a-478d-ac4c-a1e67e263573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb5f92f2-d2fd-4bb2-ac65-f9a402c64371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径和参数设置（保持不变）\n",
    "DATA_PATH = os.path.join(os.getcwd(),'data')\n",
    "actions = np.array(['curl', 'press', 'squat'])\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "no_sequences = 5\n",
    "start_folder = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c4be2bb-f7cc-4c9e-b1b7-4e9e62bc46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置每个视频序列的帧数\n",
    "sequence_length = 30  # 假设每个视频序列包含 30 帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebd7d127-1094-476f-83eb-1982d107eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据文件夹\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "for action in actions:\n",
    "    dir_path = os.path.join(DATA_PATH, action)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    for sequence in range(no_sequences):\n",
    "        sequence_path = os.path.join(dir_path, str(sequence))\n",
    "        if not os.path.exists(sequence_path):\n",
    "            os.makedirs(sequence_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beafcf65-2b9a-4296-b2a2-87c8139ee1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files = {'curl': 'videos/curls.mp4', 'press': 'videos/press.mp4', 'squat': 'videos/squats.mp4'}  # 替换为实际视频路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54e830c0-0ef0-43f0-b76c-af8cd88d5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707165639.581517       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1\n"
     ]
    }
   ],
   "source": [
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    for action, video_file in video_files.items():\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frame_count = 0\n",
    "        sequence = 0\n",
    "\n",
    "        while cap.isOpened() and sequence < no_sequences:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            image, results = mediapipe_detection(frame, pose)\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "            frame_count += 1\n",
    "            if frame_count == sequence_length:\n",
    "                sequence += 1\n",
    "                frame_count = 0\n",
    "\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5f47c-3adf-4248-ad0f-91e98ec6b291",
   "metadata": {},
   "source": [
    "processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bd51c57-62b0-414b-bb30-6a8138a8f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f10f26-fd6b-4732-9f89-1b85ca0e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    # 获取动作文件夹的路径\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    \n",
    "    # 过滤掉非数字的文件夹名（排除如 .DS_Store 的文件）\n",
    "    sequence_dirs = [dir_name for dir_name in os.listdir(action_path) if dir_name.isdigit()]\n",
    "\n",
    "    # 现在可以安全地将目录名转换为整数\n",
    "    for sequence in np.array(sequence_dirs).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):         \n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)  \n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a19e70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Map: {'curl': 0, 'press': 1, 'squat': 2}\n",
      "Labels Sample: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
      "Total number of labels: 15\n"
     ]
    }
   ],
   "source": [
    "# 检查 label_map 是否包含所有动作\n",
    "print(\"Label Map:\", label_map)\n",
    "\n",
    "# 在加载数据后，检查 labels 列表\n",
    "print(\"Labels Sample:\", labels[:15])  # 打印前10个标签作为样本\n",
    "\n",
    "# 检查 labels 列表的长度是否与数据点总数相匹配\n",
    "print(\"Total number of labels:\", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3877eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: {0: 5, 1: 5, 2: 5}\n"
     ]
    }
   ],
   "source": [
    "# 计算每个类别的样本数量\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Class counts:\", class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7d8d5ac-f56b-45ee-b30f-e4fd571ec603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f051ef1-9c5a-4bb9-ad0a-9c87d4f58f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded labels: (15, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels_array = np.array(labels).reshape(-1, 1)\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(labels_array).toarray()  # 使用 toarray() 转换为密集矩阵\n",
    "X = np.array(sequences)\n",
    "print(\"Shape of encoded labels:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c2d333e-d286-47c2-ae8d-dee058e553e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (10, 30, 132) (10, 3)\n",
      "Testing set: (5, 30, 132) (5, 3)\n"
     ]
    }
   ],
   "source": [
    "# 分割数据集为训练集、验证集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)\n",
    "\n",
    "# 输出数据集的形状以确认\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "#print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training data: [0. 1.]\n",
      "Unique labels in test data: [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels_train = np.unique(y_train)\n",
    "unique_labels_test = np.unique(y_test)\n",
    "print(\"Unique labels in training data:\", unique_labels_train)\n",
    "print(\"Unique labels in test data:\", unique_labels_test)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "995673a6-6119-4983-bc40-e01c035a0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3337943-9754-464f-bb4f-14f272cac662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_size: torch.Size([5, 30, 132])\n",
      "test_data_size: torch.Size([5, 30, 132])\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "test_label_size: torch.Size([5, 3])\n",
      "n_data_size_test: 5\n",
      "train_data_size: torch.Size([10, 30, 132])\n",
      "train_label_size: torch.Size([10, 3])\n",
      "n_data_size_train: 10\n"
     ]
    }
   ],
   "source": [
    "tensor_X_test = torch.from_numpy(X_test)\n",
    "print('test_data_size:', tensor_X_test.size())\n",
    "tensor_X_test = torch.from_numpy(X_test)\n",
    "print('test_data_size:',tensor_X_test.size())\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "tensor_y_test = torch.from_numpy(y_test)\n",
    "print('test_label_size:',tensor_y_test.size())\n",
    "n_data_size_test = tensor_X_test.size()[0]\n",
    "\n",
    "\n",
    "\n",
    "print('n_data_size_test:',n_data_size_test)\n",
    "\n",
    "tensor_X_train = torch.from_numpy(X_train)\n",
    "print('train_data_size:',tensor_X_train.size())\n",
    "tensor_y_train = torch.from_numpy(y_train)\n",
    "print('train_label_size:',tensor_y_train.size())\n",
    "n_data_size_train = tensor_X_train.size()[0]\n",
    "print('n_data_size_train:',n_data_size_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98194093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7e0a6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设您的标签数组是 tensor_y_train\n",
    "unique_labels = np.unique(tensor_y_train.numpy())\n",
    "print(\"Unique labels in the dataset:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "413c330e-d6d6-4390-9f4c-348cd357ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim,hidden_dim,layer_num,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        self.bn = nn.BatchNorm1d(30)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.bn(inputs)\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c86b4c6e-147c-4351-aa61-b23fe73321c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(132, 128, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (bn): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128  # 隐藏层维度\n",
    "n_joints = 132  # 输入维度，每帧的特征数量\n",
    "n_categories = 3  # 输出维度，类别数量\n",
    "n_layer = 3  # LSTM 层数\n",
    "rnn = LSTM(n_joints,n_hidden,n_categories,n_layer)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0e0507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['curls','press','squats']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ad95cfd6-ff38-472b-ac7f-083a89016107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return LABELS[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cce670-647b-45b6-bfe6-30c7485220f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec88ddac-5ae3-43ae-ac48-8a7d327a3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def randomTrainingExampleBatch(batch_size, flag, num=-1):\n",
    "    if flag == 'train':\n",
    "        X = tensor_X_train\n",
    "        y = tensor_y_train\n",
    "        data_size = n_data_size_train\n",
    "    elif flag == 'test':\n",
    "        X = tensor_X_test\n",
    "        y = tensor_y_test\n",
    "        data_size = n_data_size_test\n",
    "    \n",
    "    # 确保不会尝试从小于批量大小的数据集中提取数据\n",
    "    if data_size < batch_size:\n",
    "        raise ValueError(\"Data size is smaller than batch size\")\n",
    "    \n",
    "    if num == -1:\n",
    "        ran_num = random.randint(0, data_size - batch_size)\n",
    "    else:\n",
    "        ran_num = num\n",
    "    \n",
    "    # print(\"ran_num_2: %d, %d, %d\" % (ran_num, data_size - batch_size, data_size))\n",
    "    pose_sequence_tensor = X[ran_num:(ran_num + batch_size)]\n",
    "    category_tensor = y[ran_num:ran_num + batch_size, :]\n",
    "    return category_tensor.long(), pose_sequence_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7df1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9eb093ea-6974-4f7c-a554-5b77cbf35f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "learning_rate = 0.0005\n",
    "optimizer = optim.SGD(rnn.parameters(),lr=learning_rate,momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f351bad1-9629-4ef3-bbf2-7b80debdb6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 5% (0m 40s) 0.0900  / press ✓\n",
      "2000 10% (0m 56s) 0.0141  / press ✓\n",
      "3000 15% (1m 12s) 0.0093  / curls ✓\n",
      "4000 20% (1m 31s) 0.0050  / squats ✓\n",
      "5000 25% (1m 47s) 0.0037  / squats ✓\n",
      "6000 30% (2m 4s) 0.0034  / curls ✓\n",
      "7000 35% (2m 20s) 0.0023  / squats ✓\n",
      "8000 40% (2m 40s) 0.0025  / curls ✓\n",
      "9000 45% (2m 56s) 0.0022  / curls ✓\n",
      "10000 50% (3m 12s) 0.0015  / squats ✓\n",
      "11000 55% (3m 29s) 0.0013  / squats ✓\n",
      "12000 60% (3m 46s) 0.0012  / press ✓\n",
      "13000 65% (4m 1s) 0.0011  / press ✓\n",
      "14000 70% (4m 17s) 0.0012  / press ✓\n",
      "15000 75% (4m 34s) 0.0011  / curls ✓\n",
      "16000 80% (4m 51s) 0.0011  / curls ✓\n",
      "17000 85% (5m 8s) 0.0008  / press ✓\n",
      "18000 90% (5m 26s) 0.0009  / curls ✓\n",
      "19000 95% (5m 42s) 0.0008  / squats ✓\n",
      "20000 100% (5m 59s) 0.0006  / squats ✓\n"
     ]
    }
   ],
   "source": [
    "n_iters = 20000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "batch_size = 3\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category_tensor, input_sequence = randomTrainingExampleBatch(batch_size, 'train')\n",
    "    input_sequence = input_sequence.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "\n",
    "    # 获取类别索引\n",
    "    _, category_tensor = category_tensor.max(dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = rnn(input_sequence.float())\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    current_loss += loss.item()\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == LABELS[category_tensor[0]] else '✗ (%s)' % LABELS[category_tensor[0]]\n",
    "        print('%d %d%% (%s) %.4f  / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d76bd4-afb9-4fd8-9f7a-6308676a27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(),'lstm.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87398f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(flag):\n",
    "    if flag == 'train':\n",
    "        n = n_data_size_train\n",
    "    elif flag == 'test':\n",
    "        n = n_data_size_test\n",
    "\n",
    "    with torch.no_grad():\n",
    "        right = 0\n",
    "        start = time.time()\n",
    "        for i in range(n):\n",
    "            category_tensor, inputs = randomTrainingExampleBatch(1, flag, i)\n",
    "            category_index = category_tensor[0].nonzero(as_tuple=True)[0].item()\n",
    "            category = LABELS[category_index]\n",
    "\n",
    "            inputs = inputs.to(device).float()\n",
    "            output = rnn(inputs)\n",
    "            \n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess_i == category_index else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f / %s %s' % (i + 1, (i + 1) / n * 100, timeSince(start), output[0][guess_i], guess, correct))\n",
    "            \n",
    "            if category_index == guess_i:\n",
    "                right += 1\n",
    "\n",
    "    print(flag, 'accuracy', right / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e34bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(flag):\n",
    "#     if flag == 'train':\n",
    "#         n = n_data_size_train\n",
    "#     elif flag == 'test':\n",
    "#         n = n_data_size_test\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         right = 0\n",
    "#         for i in range(n):\n",
    "#             category_tensor, inputs = randomTrainingExampleBatch(1, flag, i)\n",
    "#             category_index = category_tensor[0].nonzero(as_tuple=True)[0].item()\n",
    "#             category = LABELS[category_index]\n",
    "\n",
    "#             # 确保 inputs 在正确的设备上，并且是正确的数据类型\n",
    "#             inputs = inputs.to(device).float()\n",
    "#             output = rnn(inputs)\n",
    "            \n",
    "#             guess, guess_i = categoryFromOutput(output)\n",
    "#             if category_index == guess_i:\n",
    "#                 right += 1\n",
    "\n",
    "#     print(flag, 'accuracy', right / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20% (0m 0s) 5.0628 / curls ✓\n",
      "2 40% (0m 0s) 5.0896 / press ✓\n",
      "3 60% (0m 0s) 5.0983 / press ✓\n",
      "4 80% (0m 0s) 5.0645 / curls ✓\n",
      "5 100% (0m 0s) 5.3155 / squats ✓\n",
      "test accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "test(flag='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dcfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10% (0m 0s) 5.0680 / curls ✓\n",
      "2 20% (0m 0s) 5.0733 / curls ✓\n",
      "3 30% (0m 0s) 5.6990 / squats ✓\n",
      "4 40% (0m 0s) 5.0662 / curls ✓\n",
      "5 50% (0m 0s) 5.7036 / squats ✓\n",
      "6 60% (0m 0s) 5.0956 / press ✓\n",
      "7 70% (0m 0s) 5.0793 / press ✓\n",
      "8 80% (0m 0s) 5.7000 / squats ✓\n",
      "9 90% (0m 0s) 5.6773 / squats ✓\n",
      "10 100% (0m 0s) 5.0941 / press ✓\n",
      "train accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "test(flag='train')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
